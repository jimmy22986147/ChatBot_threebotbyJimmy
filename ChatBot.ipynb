{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings  \n",
    "warnings.filterwarnings(\"ignore\")  \n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import jieba\n",
    "from jieba import lcut\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import sys\n",
    "import telepot\n",
    "import keyring\n",
    "from telepot.loop import MessageLoop\n",
    "from pprint import pprint\n",
    "import requests\n",
    "\n",
    "#plt  \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "#keras\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input, Dense, LSTM, TimeDistributed, Bidirectional, Dropout, Concatenate, RepeatVector, Activation, Dot\n",
    "from keras.layers import concatenate, dot                    \n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.initializers import TruncatedNormal\n",
    "import pydot\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = 'C://Users//user//Desktop//project//ChatBot_threebotbyJimmy//data_chatbot//'\n",
    "\n",
    "question = np.load(main_path+'pad_question.npy', allow_pickle=True)\n",
    "answer = np.load(main_path+'pad_answer.npy', allow_pickle=True)\n",
    "answer_o = np.load(main_path+'answer_o.npy', allow_pickle=True)\n",
    "                   \n",
    "with open(main_path+'vocab_bag.pkl', 'rb') as f:\n",
    "    words = pickle.load(f)\n",
    "with open(main_path+'pad_word_to_index.pkl', 'rb') as f:\n",
    "    word_to_index = pickle.load(f)\n",
    "with open(main_path+'pad_index_to_word.pkl', 'rb') as f:\n",
    "    index_to_word = pickle.load(f)\n",
    "vocab_size = len(word_to_index) + 1\n",
    "maxLen=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train(batch_size):\n",
    "    print('\\n*********************************generate_train()*********************************')\n",
    "    steps=0\n",
    "    question_ = question\n",
    "    answer_ = answer\n",
    "    while True:\n",
    "        batch_answer_o = answer_o[steps:steps+batch_size]\n",
    "        batch_question = question_[steps:steps+batch_size]\n",
    "        batch_answer = answer_[steps:steps+batch_size]\n",
    "        outs = np.zeros([batch_size, maxLen, vocab_size], dtype='float32')\n",
    "        for pos, i in enumerate(batch_answer_o):\n",
    "            for pos_, j in enumerate(i):\n",
    "                if pos_ > 20:\n",
    "                    print(i)\n",
    "                outs[pos, pos_, j] = 1 # one-hot\n",
    "        yield [batch_question, batch_answer], outs\n",
    "        steps += batch_size\n",
    "        if steps == 100000:\n",
    "            steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\finlab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\finlab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\finlab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\finlab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\finlab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\finlab\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda3\\envs\\finlab\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "truncatednormal = TruncatedNormal(mean=0.0, stddev=0.05)\n",
    "embed_layer = Embedding(input_dim=vocab_size, \n",
    "                        output_dim=100, \n",
    "                        mask_zero=True,\n",
    "                        input_length=None,\n",
    "                        embeddings_initializer= truncatednormal)\n",
    "LSTM_encoder = LSTM(512,\n",
    "                      return_sequences=True,\n",
    "                      return_state=True,\n",
    "                      kernel_initializer= 'lecun_uniform',\n",
    "                      name='encoder_lstm'\n",
    "                        )\n",
    "LSTM_decoder = LSTM(512, \n",
    "                    return_sequences=True, \n",
    "                    return_state=True, \n",
    "                    kernel_initializer= 'lecun_uniform',\n",
    "                    name='decoder_lstm'\n",
    "                   )\n",
    "\n",
    "#encoder输入 与 decoder输入\n",
    "input_question = Input(shape=(None, ), dtype='int32', name='input_question')\n",
    "input_answer = Input(shape=(None, ), dtype='int32', name='input_answer')\n",
    "\n",
    "input_question_embed = embed_layer(input_question)\n",
    "input_answer_embed = embed_layer(input_answer)\n",
    "\n",
    "\n",
    "encoder_lstm, question_h, question_c = LSTM_encoder(input_question_embed)\n",
    "\n",
    "decoder_lstm, _, _ = LSTM_decoder(input_answer_embed, \n",
    "                                  initial_state=[question_h, question_c])\n",
    "\n",
    "attention = dot([decoder_lstm, encoder_lstm], axes=[2, 2])\n",
    "attention = Activation('softmax')(attention)\n",
    "context = dot([attention, encoder_lstm], axes=[2,1])\n",
    "decoder_combined_context = concatenate([context, decoder_lstm])\n",
    "\n",
    "\n",
    "# Has another weight + tanh layer as described in equation (5) of the paper\n",
    "decoder_dense1 = TimeDistributed(Dense(256,activation=\"tanh\"))\n",
    "decoder_dense2 = TimeDistributed(Dense(vocab_size,activation=\"softmax\"))\n",
    "output = decoder_dense1(decoder_combined_context) # equation (5) of the paper\n",
    "output = decoder_dense2(output) # equation (6) of the paper\n",
    "\n",
    "model = Model([input_question, input_answer], output)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "model.load_weights(main_path+'models/W--184-0.5949-.h5')\n",
    "#model.load_weights('models/W--200-0.6064-.h5')\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_model = Model(input_question, [encoder_lstm, question_h, question_c])\n",
    "#question_model.summary()\n",
    "answer_h = Input(shape=(512,))\n",
    "answer_c = Input(shape=(512,))\n",
    "encoder_lstm = Input(shape=(maxLen,512))\n",
    "target, h, c = LSTM_decoder(input_answer_embed, initial_state=[answer_h, answer_c])\n",
    "attention = dot([target, encoder_lstm], axes=[2, 2])\n",
    "attention_ = Activation('softmax')(attention)\n",
    "context = dot([attention_, encoder_lstm], axes=[2,1])\n",
    "decoder_combined_context = concatenate([context, target])\n",
    "output = decoder_dense1(decoder_combined_context) # equation (5) of the paper\n",
    "output = decoder_dense2(output) # equation (6) of the paper\n",
    "answer_model = Model([input_answer, answer_h, answer_c, encoder_lstm], [output, h, c, attention_])\n",
    "#answer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_weather(city):\n",
    "    #TODO: Get weather by api\n",
    "    url = 'http://wthrcdn.etouch.cn/weather_mini?city=' + city\n",
    "    page = requests.get(url)\n",
    "    data = page.json()\n",
    "    temperature = data['data']['wendu']\n",
    "    notice = data['data']['ganmao']\n",
    "    outstrs = \"地点： %s\\n气温： %s\\n注意： %s\" % (city, temperature, notice)\n",
    "    return outstrs + ' EOS'\n",
    "def input_question(seq):\n",
    "    seq = jieba.lcut(seq.strip(), cut_all=False)\n",
    "    sentence = seq\n",
    "    try:\n",
    "        seq = np.array([word_to_index[w] for w in seq])\n",
    "    except KeyError:\n",
    "        seq = np.array([36874, 165, 14625])\n",
    "    seq = sequence.pad_sequences([seq], maxlen=maxLen,\n",
    "                                          padding='post', truncating='post')\n",
    "    #print(seq)\n",
    "    return seq, sentence\n",
    "def decode_greedy(seq, sentence):\n",
    "    question = seq\n",
    "    for index in question[0]:\n",
    "        if int(index) == 5900:\n",
    "            for index_ in question[0]:\n",
    "                if index_ in [7851, 11842,2406, 3485, 823, 12773, 8078]:\n",
    "                    return act_weather(index_to_word[index_])\n",
    "    answer = np.zeros((1, 1))\n",
    "    attention_plot = np.zeros((20, 20))\n",
    "    answer[0, 0] = word_to_index['BOS']\n",
    "    i=1\n",
    "    answer_ = []\n",
    "    flag = 0\n",
    "    encoder_lstm_, question_h, question_c = question_model.predict(x=question, verbose=1)\n",
    "#     print(question_h, '\\n')\n",
    "    while flag != 1:\n",
    "        prediction, prediction_h, prediction_c, attention = answer_model.predict([\n",
    "            answer, question_h, question_c, encoder_lstm_\n",
    "        ])\n",
    "        attention_weights = attention.reshape(-1, )\n",
    "        attention_plot[i] = attention_weights\n",
    "        word_arg = np.argmax(prediction[0, -1, :])#\n",
    "        answer_.append(index_to_word[word_arg])\n",
    "        if word_arg == word_to_index['EOS']  or i > 20:\n",
    "            flag = 1\n",
    "        answer = np.zeros((1, 1))\n",
    "        answer[0, 0] = word_arg\n",
    "        question_h = prediction_h\n",
    "        question_c = prediction_c\n",
    "        i += 1\n",
    "    result = ' '.join(answer_)\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence)]\n",
    "    #plot_attention(attention_plot, sentence, result.split(' '))\n",
    "    return ' '.join(answer_)\n",
    "def decode_beamsearch(seq, beam_size):\n",
    "    question = seq\n",
    "    encoder_lstm_, question_h, question_c = question_model.predict(x=question, verbose=1)\n",
    "    sequences = [[[word_to_index['BOS']], 1.0, question_h, question_c]]\n",
    "    answer = np.zeros((1, 1))\n",
    "    answer[0, 0] = word_to_index['BOS']\n",
    "    answer_ = ''\n",
    "    flag = 0\n",
    "    last_words = [word_to_index['BOS']]\n",
    "    for i in range(maxLen):\n",
    "        all_candidates = []\n",
    "        for j in range(len(sequences)):\n",
    "            s, score, h, c = sequences[j]\n",
    "            last_word = s[-1]\n",
    "            if not isinstance(last_word, int):\n",
    "                last_word=last_word[-1]\n",
    "            answer[0, 0] = last_word\n",
    "            output, h, c, _ = answer_model.predict([answer, h, c, encoder_lstm_])\n",
    "            output = output[0, -1]\n",
    "            for k in range(len(output)):\n",
    "                candidate = [seq+[k], score*-np.log(output[k]), h, c]\n",
    "            all_candidates.append(candidate)\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "        sequences = ordered[:beam_size]\n",
    "    answer_ = sequences[0][0]\n",
    "    #print(answer_[0])\n",
    "    answer_ = [index_to_word[x] for x in answer_[0] if (x!=0)]\n",
    "    answer_ = ' '.join(answer_)\n",
    "    return answer_\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    zhfont = matplotlib.font_manager.FontProperties(fname='simkai.ttf')\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    attention = [x[::-1] for x in attention]\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    fontdict = {'fontsize': 20}\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict,fontproperties=zhfont)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict, fontproperties=zhfont)\n",
    "#     ax.yaxis.set_ticks_position('right') #y轴刻度位置靠右\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chat_Bot(msg):\n",
    "    seq = msg\n",
    "    seq, sentence = input_question(seq)\n",
    "    answer = decode_greedy(seq, sentence)   \n",
    "    answer = 'Boss您好,'+ answer.replace('EOS', '')\n",
    "    answer = answer.replace(' ', '')    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\user\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.528 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Boss您好,我是你的老婆啊'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chat_Bot('你是谁')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finlab",
   "language": "python",
   "name": "finlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
